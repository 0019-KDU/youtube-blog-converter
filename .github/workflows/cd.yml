name: Continuous Deployment

on:
  workflow_run:
    workflows: ["Continuous Integration"]
    types:
      - completed
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging
      skip_tests:
        description: 'Skip security and performance tests'
        required: false
        default: false
        type: boolean

# Environment variables for CD
env:
  # Security & Authentication
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SUPADATA_API_KEY: ${{ secrets.SUPADATA_API_KEY }}
  JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
  JWT_ACCESS_TOKEN_EXPIRES: ${{ secrets.JWT_ACCESS_TOKEN_EXPIRES }}
  MONGODB_URI: ${{ secrets.MONGODB_URI }}
  MONGODB_DB_NAME: ${{ secrets.MONGODB_DB_NAME }}
  FLASK_SECRET_KEY: ${{ secrets.FLASK_SECRET_KEY }}

  # Container Registry
  DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
  CONTAINER_REGISTRY: docker.io
  CONTAINER_IMAGE: youtube-blog-converter

  # Application Settings
  FLASK_DEBUG: "false"
  FLASK_HOST: "0.0.0.0"
  OPENAI_MODEL_NAME: ${{ secrets.OPENAI_MODEL_NAME || 'gpt-3.5-turbo' }}
  GA_MEASUREMENT_ID: ${{ secrets.GA_MEASUREMENT_ID }}

  # Monitoring
  PROMETHEUS_ENDPOINT: ${{ secrets.PROMETHEUS_ENDPOINT }}
  GRAFANA_ENDPOINT: ${{ secrets.GRAFANA_ENDPOINT }}
  LOKI_ENDPOINT: ${{ secrets.LOKI_ENDPOINT }}
  MONITORING_ENABLED: "true"
  METRICS_PORT: "8000"

  # Performance Testing
  JMETER_THREADS: ${{ vars.JMETER_THREADS || '50' }}
  JMETER_RAMPUP: ${{ vars.JMETER_RAMPUP || '60' }}
  JMETER_DURATION: ${{ vars.JMETER_DURATION || '300' }}
  JMETER_TARGET_RPS: ${{ vars.JMETER_TARGET_RPS || '100' }}

  # Security Testing
  ZAP_SCAN_DURATION: ${{ vars.ZAP_SCAN_DURATION || '10' }}
  ZAP_SPIDER_DURATION: ${{ vars.ZAP_SPIDER_DURATION || '3' }}

  # Azure Configuration
  PRODUCTION_URL: https://yt-agent-fjdbh7dndpcjh4dc.centralus-01.azurewebsites.net
  GREEN_URL: https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net

permissions:
  contents: read
  id-token: write
  attestations: write

jobs:
  # Check if CI was successful before proceeding
  check-ci-status:
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch'
    outputs:
      should-deploy: ${{ steps.check.outputs.should-deploy }}
    steps:
      - name: Check CI Status
        id: check
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "Manual deployment triggered"
            echo "should-deploy=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]]; then
            echo "CI pipeline completed successfully"
            echo "should-deploy=true" >> $GITHUB_OUTPUT
          else
            echo "CI pipeline failed, skipping deployment"
            echo "should-deploy=false" >> $GITHUB_OUTPUT
          fi

  deploy-to-dockerhub:
    runs-on: ubuntu-latest
    needs: [check-ci-status]
    if: needs.check-ci-status.outputs.should-deploy == 'true'
    outputs:
      image-tag: ${{ steps.get-tag.outputs.tag }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Get latest image tag
        id: get-tag
        run: |
          # Get the latest tag from the CI workflow or use main
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            TAG="main-$(git rev-parse --short HEAD)"
          else
            TAG="main-$(echo ${{ github.sha }} | cut -c1-7)"
          fi
          echo "tag=$TAG" >> $GITHUB_OUTPUT
          echo "Using image tag: $TAG"

      - name: Verify Docker image exists
        run: |
          echo "ðŸ” Verifying Docker image exists..."
          IMAGE="${{ env.CONTAINER_REGISTRY }}/${{ secrets.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ steps.get-tag.outputs.tag }}"

          # Login to verify image exists
          echo "${{ secrets.DOCKERHUB_TOKEN }}" | docker login -u "${{ secrets.DOCKERHUB_USERNAME }}" --password-stdin

          if docker manifest inspect "$IMAGE" > /dev/null 2>&1; then
            echo "âœ… Image $IMAGE exists and is ready for deployment"
          else
            echo "âŒ Image $IMAGE not found"
            exit 1
          fi

  sign-container:
    runs-on: ubuntu-latest
    needs: [deploy-to-dockerhub]
    steps:
      - name: Install Cosign
        uses: sigstore/cosign-installer@v3.4.0

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Sign container image
        run: |
          IMAGE="${{ env.CONTAINER_REGISTRY }}/${{ secrets.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ needs.deploy-to-dockerhub.outputs.image-tag }}"
          echo "ðŸ” Signing container image: $IMAGE"
          cosign sign --yes "$IMAGE"

      - name: Generate and attest SBOM
        run: |
          IMAGE="${{ env.CONTAINER_REGISTRY }}/${{ secrets.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ needs.deploy-to-dockerhub.outputs.image-tag }}"
          echo "ðŸ“‹ Generating and attesting SBOM for: $IMAGE"

          # Generate SBOM
          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
            anchore/syft:latest "$IMAGE" -o spdx-json > sbom.spdx.json

          # Attest SBOM
          cosign attest --yes --predicate sbom.spdx.json --type spdx "$IMAGE"

  deploy-to-azure-green:
    runs-on: ubuntu-latest
    needs: [deploy-to-dockerhub, sign-container]
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Azure CLI Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Check and start green slot if needed
        run: |
          echo "ðŸ” Checking green slot status..."

          # Get current state of green slot
          STATE=$(az webapp show --name yt-agent --resource-group rg-yt-agent --slot green --query "state" -o tsv)
          echo "Green slot current state: $STATE"

          if [ "$STATE" != "Running" ]; then
            echo "ðŸš€ Starting green slot..."
            az webapp start --name yt-agent --resource-group rg-yt-agent --slot green

            # Wait for slot to be running
            for i in {1..10}; do
              sleep 30
              STATE=$(az webapp show --name yt-agent --resource-group rg-yt-agent --slot green --query "state" -o tsv)
              echo "Green slot state check $i: $STATE"
              if [ "$STATE" == "Running" ]; then
                echo "âœ… Green slot is now running"
                break
              fi
            done

            if [ "$STATE" != "Running" ]; then
              echo "âŒ Failed to start green slot"
              exit 1
            fi
          else
            echo "âœ… Green slot is already running"
          fi

      - name: Deploy to Azure Web App (Green Slot)
        uses: azure/webapps-deploy@v3
        with:
          app-name: yt-agent
          slot-name: green
          images: ${{ env.CONTAINER_REGISTRY }}/${{ secrets.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ needs.deploy-to-dockerhub.outputs.image-tag }}

      - name: Wait for green deployment to be ready
        run: |
          echo "â³ Waiting for green deployment to be ready..."

          for i in {1..20}; do
            if curl -sf "${{ env.GREEN_URL }}/health" > /dev/null; then
              echo "âœ… Green deployment is ready (attempt $i)"
              break
            else
              echo "â³ Green deployment not ready yet (attempt $i/20)"
              sleep 30
            fi
          done

          # Final check
          if curl -sf "${{ env.GREEN_URL }}/health" > /dev/null; then
            echo "âœ… Green deployment is healthy and ready"
          else
            echo "âŒ Green deployment failed health check"
            exit 1
          fi

  validate-green-deployment:
    runs-on: ubuntu-latest
    needs: [deploy-to-azure-green]
    steps:
      - name: Validate Green Deployment
        run: |
          echo "ðŸ” Validating green deployment..."

          # Health check
          if curl -sf "${{ env.GREEN_URL }}/health" | jq -e '.status == "healthy"' > /dev/null; then
            echo "âœ… Health check passed"
          else
            echo "âŒ Health check failed"
            exit 1
          fi

          # Test key endpoints
          endpoints=("/health" "/auth/login")

          for endpoint in "${endpoints[@]}"; do
            url="${{ env.GREEN_URL }}${endpoint}"
            if curl -sf "$url" > /dev/null; then
              echo "âœ… Endpoint $endpoint is responding"
            else
              echo "âŒ Endpoint $endpoint is not responding"
              exit 1
            fi
          done

          echo "âœ… All validation checks passed"

  owasp-zap-baseline-scan:
    runs-on: ubuntu-latest
    needs: validate-green-deployment
    if: github.event.inputs.skip_tests != 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create ZAP configuration
        run: |
          cat > zap.conf << 'EOF'
          # ZAP Baseline Configuration
          spider.maxDuration=${{ env.ZAP_SPIDER_DURATION }}

          # Authentication configuration (if needed)
          # auth.method=form
          # auth.loginurl=/login
          # auth.username=testuser
          # auth.password=testpass
          EOF

      - name: Run OWASP ZAP Baseline Scan
        run: |
          # Run ZAP Docker container for baseline scan
          echo "ðŸ” Starting OWASP ZAP baseline scan..."

          # ZAP baseline scan exit codes:
          # 0 = Success (no alerts found)
          # 1 = At least 1 FAIL (high risk) alert found
          # 2 = At least 1 WARN (medium risk) alert found
          # 3 = At least 1 INFO (low risk) alert found
          set +e  # Don't exit on non-zero exit codes
          docker run --rm \
            -v $(pwd):/zap/wrk/ \
            -t ghcr.io/zaproxy/zaproxy:stable \
            zap-baseline.py \
            -t "${{ env.GREEN_URL }}" \
            -d \
            -m ${{ env.ZAP_SCAN_DURATION }} \
            -z "-config api.disablekey=true" \
            -J zap-baseline-report.json \
            -w zap-baseline-report.md \
            -r zap-baseline-report.html

          ZAP_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          echo "ðŸ“‹ ZAP scan completed with exit code: $ZAP_EXIT_CODE"
          case $ZAP_EXIT_CODE in
            0) echo "âœ… No security alerts found" ;;
            1) echo "ðŸ”´ HIGH RISK alerts found" ;;
            2) echo "ðŸŸ¡ MEDIUM RISK alerts found" ;;
            3) echo "ðŸ”µ LOW/INFO alerts found" ;;
            *) echo "âš ï¸ ZAP scan completed with unexpected exit code: $ZAP_EXIT_CODE" ;;
          esac

          # Verify report files were generated
          echo "ðŸ“ Verifying generated report files..."
          missing_files=0
          for format in json html md; do
            report_file="zap-baseline-report.$format"
            if [ -f "$report_file" ] && [ -s "$report_file" ]; then
              file_size=$(stat -c%s "$report_file" 2>/dev/null || stat -f%z "$report_file" 2>/dev/null || echo "unknown")
              echo "âœ… $report_file exists ($file_size bytes)"
            else
              echo "âŒ $report_file missing or empty"
              missing_files=$((missing_files + 1))
            fi
          done

          if [ $missing_files -gt 0 ]; then
            echo "âš ï¸ Some report files are missing. This may indicate a scan failure."
            echo "ðŸ“ Current directory contents:"
            ls -la | grep -E "(zap|report)" || echo "No ZAP files found"
          fi

      - name: Parse ZAP Scan Results
        id: parse-results
        if: always()
        run: |
          echo "ðŸ“Š Parsing ZAP scan results..."

          if [ -f "zap-baseline-report.json" ]; then
            # Extract key metrics
            HIGH_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("High"))] | length' zap-baseline-report.json || echo "0")
            MEDIUM_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Medium"))] | length' zap-baseline-report.json || echo "0")
            LOW_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Low"))] | length' zap-baseline-report.json || echo "0")
            INFO_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Informational"))] | length' zap-baseline-report.json || echo "0")

            echo "high_alerts=$HIGH_ALERTS" >> $GITHUB_OUTPUT
            echo "medium_alerts=$MEDIUM_ALERTS" >> $GITHUB_OUTPUT
            echo "low_alerts=$LOW_ALERTS" >> $GITHUB_OUTPUT
            echo "info_alerts=$INFO_ALERTS" >> $GITHUB_OUTPUT

            echo "ðŸ” ZAP Scan Results Summary:"
            echo "  - High Risk: $HIGH_ALERTS"
            echo "  - Medium Risk: $MEDIUM_ALERTS"
            echo "  - Low Risk: $LOW_ALERTS"
            echo "  - Informational: $INFO_ALERTS"

            # Security gate logic (fail if high > 0 or medium > 5)
            if [ "$HIGH_ALERTS" -eq 0 ] && [ "$MEDIUM_ALERTS" -le 5 ]; then
              echo "security_gate_passed=true" >> $GITHUB_OUTPUT
              echo "âœ… Security gate passed"
            else
              echo "security_gate_passed=false" >> $GITHUB_OUTPUT
              echo "âŒ Security gate failed"
            fi
          else
            echo "âŒ No ZAP report found"
            echo "high_alerts=unknown" >> $GITHUB_OUTPUT
            echo "medium_alerts=unknown" >> $GITHUB_OUTPUT
            echo "low_alerts=unknown" >> $GITHUB_OUTPUT
            echo "info_alerts=unknown" >> $GITHUB_OUTPUT
            echo "security_gate_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Security Summary Report
        id: generate-summary
        if: always()
        run: |
          # Generate comprehensive security summary
          cat > security-summary.md << 'EOF'
          # ðŸ”’ OWASP ZAP Security Scan Report

          ## Scan Details
          - **Target:** ${{ env.GREEN_URL }}
          - **Scan Type:** Baseline Security Scan
          - **Duration:** ${{ env.ZAP_SCAN_DURATION }} minutes
          - **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          - **Branch:** ${{ github.ref_name }}
          - **Commit:** ${{ github.sha }}
          - **ZAP Version:** Latest Stable

          ## Results Summary
          - **High Risk:** ${{ steps.parse-results.outputs.high_alerts }} alerts
          - **Medium Risk:** ${{ steps.parse-results.outputs.medium_alerts }} alerts
          - **Low Risk:** ${{ steps.parse-results.outputs.low_alerts }} alerts
          - **Informational:** ${{ steps.parse-results.outputs.info_alerts }} alerts

          ## Security Gate Status
          **Status:** ${{ steps.parse-results.outputs.security_gate_passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}

          ### Gate Criteria
          - High Risk alerts: 0 (current: ${{ steps.parse-results.outputs.high_alerts }})
          - Medium Risk alerts: â‰¤ 5 (current: ${{ steps.parse-results.outputs.medium_alerts }})

          ## Scan Coverage
          This baseline scan performed the following security tests:
          - **Passive Scanning:** Analyzed HTTP responses for security issues
          - **Spider/Crawler:** Discovered application endpoints and pages
          - **Authentication:** Tested for authentication bypass vulnerabilities
          - **Session Management:** Verified session handling security
          - **Input Validation:** Checked for injection vulnerabilities
          - **SSL/TLS Configuration:** Analyzed encryption and certificate setup

          ## Recommendations
          ${{ steps.parse-results.outputs.security_gate_passed == 'true' &&
             '- Continue with deployment process
             - Monitor security alerts in production
             - Schedule regular security scans' ||
             '- Review and fix high/medium risk vulnerabilities
             - Re-run security scan before deployment
             - Consider implementing additional security controls' }}

          ## Files Generated
          - **JSON Report:** `zap-baseline-report.json`
          - **HTML Report:** `zap-baseline-report.html`
          - **Markdown Report:** `zap-baseline-report.md`

          *This report was automatically generated by the CI/CD pipeline.*
          EOF

          cat security-summary.md

      - name: List Generated Files
        if: always()
        run: |
          echo "ðŸ“ Listing generated ZAP report files:"
          ls -la zap-baseline-report.* security-summary.md || true
          echo "ðŸ“‹ File details:"
          for file in zap-baseline-report.json zap-baseline-report.html zap-baseline-report.md security-summary.md; do
            if [ -f "$file" ]; then
              echo "âœ… $file exists ($(stat -c%s "$file") bytes)"
            else
              echo "âŒ $file missing"
            fi
          done

      - name: Upload ZAP Scan Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: owasp-zap-reports
          path: |
            zap-baseline-report.*
            security-summary.md
          retention-days: 30

      - name: Security Gate Evaluation
        id: security-gate
        if: always()
        run: |
          if [ "${{ steps.parse-results.outputs.security_gate_passed }}" = "true" ]; then
            echo "âœ… Security gate passed - proceeding with deployment"
            echo "gate_passed=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Security gate failed - blocking deployment"
            echo "gate_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

  jmeter-load-test:
    runs-on: ubuntu-latest
    needs: [owasp-zap-baseline-scan]
    if: github.event.inputs.skip_tests != 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JMeter
        run: |
          echo "ðŸ“¦ Setting up Apache JMeter..."
          wget -q https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.6.3.tgz
          tar -xzf apache-jmeter-5.6.3.tgz
          export PATH=$PATH:$(pwd)/apache-jmeter-5.6.3/bin
          echo "$(pwd)/apache-jmeter-5.6.3/bin" >> $GITHUB_PATH

      - name: Create JMeter Test Plan
        run: |
          cat > load-test.jmx << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.3">
            <hashTree>
              <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Load Test Plan">
                <elementProp name="TestPlan.arguments" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables">
                  <collectionProp name="Arguments.arguments"/>
                </elementProp>
                <stringProp name="TestPlan.user_define_classpath"></stringProp>
                <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
                <boolProp name="TestPlan.functional_mode">false</boolProp>
              </TestPlan>
              <hashTree>
                <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Thread Group">
                  <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
                  <elementProp name="ThreadGroup.main_controller" elementType="LoopController" guiclass="LoopControllerGui" testclass="LoopController" testname="Loop Controller">
                    <boolProp name="LoopController.continue_forever">false</boolProp>
                    <intProp name="LoopController.loops">-1</intProp>
                  </elementProp>
                  <stringProp name="ThreadGroup.num_threads">${{ env.JMETER_THREADS }}</stringProp>
                  <stringProp name="ThreadGroup.ramp_time">${{ env.JMETER_RAMPUP }}</stringProp>
                  <stringProp name="ThreadGroup.duration">${{ env.JMETER_DURATION }}</stringProp>
                  <stringProp name="ThreadGroup.delay"></stringProp>
                  <boolProp name="ThreadGroup.scheduler">true</boolProp>
                </ThreadGroup>
                <hashTree>
                  <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Health Check">
                    <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="HTTPArgumentsPanel" testclass="Arguments" testname="User Defined Variables">
                      <collectionProp name="Arguments.arguments"/>
                    </elementProp>
                    <stringProp name="HTTPSampler.domain">${__P(host,yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net)}</stringProp>
                    <stringProp name="HTTPSampler.port">443</stringProp>
                    <stringProp name="HTTPSampler.protocol">https</stringProp>
                    <stringProp name="HTTPSampler.contentEncoding"></stringProp>
                    <stringProp name="HTTPSampler.path">/health</stringProp>
                    <stringProp name="HTTPSampler.method">GET</stringProp>
                    <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                    <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                    <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                    <boolProp name="HTTPSampler.DO_MULTIPART_POST">false</boolProp>
                    <stringProp name="HTTPSampler.embedded_url_re"></stringProp>
                    <stringProp name="HTTPSampler.connect_timeout"></stringProp>
                    <stringProp name="HTTPSampler.response_timeout"></stringProp>
                  </HTTPSamplerProxy>
                  <hashTree/>
                  <ResultCollector guiclass="SummaryReport" testclass="ResultCollector" testname="Summary Report">
                    <boolProp name="ResultCollector.error_logging">false</boolProp>
                    <objProp>
                      <name>saveConfig</name>
                      <value class="SampleSaveConfiguration">
                        <time>true</time>
                        <latency>true</latency>
                        <timestamp>true</timestamp>
                        <success>true</success>
                        <label>true</label>
                        <code>true</code>
                        <message>true</message>
                        <threadName>true</threadName>
                        <dataType>true</dataType>
                        <encoding>false</encoding>
                        <assertions>true</assertions>
                        <subresults>true</subresults>
                        <responseData>false</responseData>
                        <samplerData>false</samplerData>
                        <xml>false</xml>
                        <fieldNames>true</fieldNames>
                        <responseHeaders>false</responseHeaders>
                        <requestHeaders>false</requestHeaders>
                        <responseDataOnError>false</responseDataOnError>
                        <saveAssertionResultsFailureMessage>true</saveAssertionResultsFailureMessage>
                        <assertionsResultsToSave>0</assertionsResultsToSave>
                        <bytes>true</bytes>
                        <sentBytes>true</sentBytes>
                        <url>true</url>
                        <threadCounts>true</threadCounts>
                        <idleTime>true</idleTime>
                        <connectTime>true</connectTime>
                      </value>
                    </objProp>
                    <stringProp name="filename">jmeter-results.jtl</stringProp>
                  </ResultCollector>
                  <hashTree/>
                </hashTree>
              </hashTree>
            </hashTree>
          </jmeterTestPlan>
          EOF

      - name: Run JMeter Load Test
        run: |
          echo "ðŸš€ Starting JMeter load test..."
          echo "Test Configuration:"
          echo "  - Threads: ${{ env.JMETER_THREADS }}"
          echo "  - Ramp-up: ${{ env.JMETER_RAMPUP }} seconds"
          echo "  - Duration: ${{ env.JMETER_DURATION }} seconds"
          echo "  - Target: ${{ env.GREEN_URL }}"

          jmeter -n -t load-test.jmx -l jmeter-results.jtl -e -o jmeter-report/ \
            -Jhost=$(echo "${{ env.GREEN_URL }}" | sed 's|https\?://||' | cut -d'/' -f1)

      - name: Parse JMeter Results
        id: parse-results
        if: always()
        run: |
          echo "ðŸ“Š Parsing JMeter test results..."

          if [ -f "jmeter-results.jtl" ]; then
            # Calculate key metrics from JTL file
            TOTAL_SAMPLES=$(tail -n +2 jmeter-results.jtl | wc -l)
            ERROR_COUNT=$(tail -n +2 jmeter-results.jtl | awk -F',' '$8=="false" {count++} END {print count+0}')
            ERROR_RATE=$(echo "scale=2; $ERROR_COUNT * 100 / $TOTAL_SAMPLES" | bc -l || echo "0")

            # Calculate percentiles from response times (column 2)
            RESPONSE_TIMES=$(tail -n +2 jmeter-results.jtl | awk -F',' '{print $2}' | sort -n)
            AVG_RESPONSE=$(echo "$RESPONSE_TIMES" | awk '{sum+=$1} END {print sum/NR}' | xargs printf "%.0f")
            P95_RESPONSE=$(echo "$RESPONSE_TIMES" | awk 'BEGIN{c=0} {a[c++]=$1} END{print a[int(c*0.95)]}')
            P99_RESPONSE=$(echo "$RESPONSE_TIMES" | awk 'BEGIN{c=0} {a[c++]=$1} END{print a[int(c*0.99)]}')

            # Calculate throughput (requests per second)
            DURATION_ACTUAL=$(tail -n +2 jmeter-results.jtl | awk -F',' 'BEGIN{min=999999999999; max=0} {if($1<min) min=$1; if($1>max) max=$1} END{print (max-min)/1000}')
            THROUGHPUT=$(echo "scale=2; $TOTAL_SAMPLES / $DURATION_ACTUAL" | bc -l || echo "0")

            echo "total_samples=$TOTAL_SAMPLES" >> $GITHUB_OUTPUT
            echo "error_count=$ERROR_COUNT" >> $GITHUB_OUTPUT
            echo "error_rate=$ERROR_RATE" >> $GITHUB_OUTPUT
            echo "avg_response_time=$AVG_RESPONSE" >> $GITHUB_OUTPUT
            echo "p95_response_time=$P95_RESPONSE" >> $GITHUB_OUTPUT
            echo "p99_response_time=$P99_RESPONSE" >> $GITHUB_OUTPUT
            echo "throughput=$THROUGHPUT" >> $GITHUB_OUTPUT

            echo "ðŸŽ¯ JMeter Test Results Summary:"
            echo "  - Total Samples: $TOTAL_SAMPLES"
            echo "  - Error Rate: $ERROR_RATE%"
            echo "  - Avg Response Time: ${AVG_RESPONSE}ms"
            echo "  - 95th Percentile: ${P95_RESPONSE}ms"
            echo "  - 99th Percentile: ${P99_RESPONSE}ms"
            echo "  - Throughput: ${THROUGHPUT} req/sec"

            # Performance gate logic
            ERROR_THRESHOLD=5.0  # 5% error rate threshold
            P95_THRESHOLD=2000   # 2 second P95 threshold

            if (( $(echo "$ERROR_RATE <= $ERROR_THRESHOLD" | bc -l) )) && \
               (( $(echo "$P95_RESPONSE <= $P95_THRESHOLD" | bc -l) )); then
              echo "performance_gate_passed=true" >> $GITHUB_OUTPUT
              echo "âœ… Performance gate passed"
            else
              echo "performance_gate_passed=false" >> $GITHUB_OUTPUT
              echo "âŒ Performance gate failed"
            fi
          else
            echo "âŒ No JMeter results found"
            echo "performance_gate_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Performance Summary Report
        id: generate-summary
        if: always()
        run: |
          cat > performance-summary.md << 'EOF'
          # ðŸ“ˆ JMeter Load Test Report

          ## Test Configuration
          - **Target:** ${{ env.GREEN_URL }}
          - **Test Type:** Load Test
          - **Threads:** ${{ env.JMETER_THREADS }}
          - **Ramp-up:** ${{ env.JMETER_RAMPUP }} seconds
          - **Duration:** ${{ env.JMETER_DURATION }} seconds
          - **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          - **Branch:** ${{ github.ref_name }}
          - **Commit:** ${{ github.sha }}

          ## Results Summary
          - **Total Samples:** ${{ steps.parse-results.outputs.total_samples }}
          - **Error Rate:** ${{ steps.parse-results.outputs.error_rate }}%
          - **Average Response Time:** ${{ steps.parse-results.outputs.avg_response_time }}ms
          - **95th Percentile:** ${{ steps.parse-results.outputs.p95_response_time }}ms
          - **99th Percentile:** ${{ steps.parse-results.outputs.p99_response_time }}ms
          - **Throughput:** ${{ steps.parse-results.outputs.throughput }} req/sec

          ## Performance Gate Status
          **Status:** ${{ steps.parse-results.outputs.performance_gate_passed == 'true' && 'âœ… PASSED' || 'âŒ FAILED' }}

          ### Gate Criteria
          - Error Rate: â‰¤ 5% (current: ${{ steps.parse-results.outputs.error_rate }}%)
          - 95th Percentile: â‰¤ 2000ms (current: ${{ steps.parse-results.outputs.p95_response_time }}ms)

          ## Recommendations
          ${{ steps.parse-results.outputs.performance_gate_passed == 'true' &&
             '- Application performance is acceptable for production
             - Continue monitoring response times in production
             - Consider capacity planning for future growth' ||
             '- Review and optimize slow endpoints
             - Consider infrastructure scaling
             - Re-run performance tests after optimization' }}

          *This report was automatically generated by the CI/CD pipeline.*
          EOF

          cat performance-summary.md

      - name: Upload JMeter Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jmeter-reports
          path: |
            jmeter-results.jtl
            jmeter-report/
            performance-summary.md
          retention-days: 30

      - name: Performance Gate Evaluation
        id: performance-gate
        if: always()
        run: |
          if [ "${{ steps.parse-results.outputs.performance_gate_passed }}" = "true" ]; then
            echo "âœ… Performance gate passed - proceeding with deployment"
            echo "gate_passed=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Performance gate failed - blocking deployment"
            echo "gate_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

  swap-to-production:
    runs-on: ubuntu-latest
    needs: [jmeter-load-test]
    environment: production
    steps:
      - name: Azure CLI Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Swap Green to Production
        run: |
          echo "ðŸ”„ Swapping green slot to production..."
          az webapp deployment slot swap \
            --name yt-agent \
            --resource-group rg-yt-agent \
            --slot green \
            --target-slot production

      - name: Verify Production Deployment
        run: |
          echo "ðŸ” Verifying production deployment..."

          # Wait for swap to complete
          sleep 30

          # Test production endpoint
          for i in {1..10}; do
            if curl -sf "${{ env.PRODUCTION_URL }}/health" | jq -e '.status == "healthy"' > /dev/null; then
              echo "âœ… Production deployment verified (attempt $i)"
              break
            else
              echo "â³ Production not ready yet (attempt $i/10)"
              sleep 30
            fi
          done

          # Final verification
          if curl -sf "${{ env.PRODUCTION_URL }}/health" | jq -e '.status == "healthy"' > /dev/null; then
            echo "âœ… Production deployment is healthy"
          else
            echo "âŒ Production deployment failed health check"
            exit 1
          fi

  rollback-if-needed:
    runs-on: ubuntu-latest
    needs: [swap-to-production]
    if: failure()
    environment: production
    steps:
      - name: Azure CLI Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Rollback Production
        run: |
          echo "ðŸ”„ Rolling back production deployment..."
          az webapp deployment slot swap \
            --name yt-agent \
            --resource-group rg-yt-agent \
            --slot production \
            --target-slot green

          echo "âœ… Rollback completed"

  cleanup-green-slot:
    runs-on: ubuntu-latest
    needs: [swap-to-production]
    if: always() && needs.swap-to-production.result == 'success'
    steps:
      - name: Azure CLI Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Stop Green Slot for Cost Optimization
        run: |
          echo "ðŸ’° Stopping green slot for cost optimization..."
          az webapp stop --name yt-agent --resource-group rg-yt-agent --slot green
          echo "âœ… Green slot stopped successfully"

  notify:
    runs-on: ubuntu-latest
    needs: [swap-to-production, rollback-if-needed, cleanup-green-slot]
    if: always()
    steps:
      - name: Notify Deployment Status
        run: |
          if [[ "${{ needs.swap-to-production.result }}" == "success" ]]; then
            echo "âœ… Deployment completed successfully!"
            echo "ðŸš€ Application is now live at ${{ env.PRODUCTION_URL }}"

            # Send success notification (webhook, Slack, etc.)
            if [[ -n "${{ env.SLACK_WEBHOOK_URL }}" ]]; then
              curl -X POST -H 'Content-type: application/json' \
                --data '{"text":"âœ… Deployment Success: YouTube Blog Converter deployed to production"}' \
                ${{ env.SLACK_WEBHOOK_URL }}
            fi
          else
            echo "âŒ Deployment failed!"

            # Send failure notification
            if [[ -n "${{ env.SLACK_WEBHOOK_URL }}" ]]; then
              curl -X POST -H 'Content-type: application/json' \
                --data '{"text":"âŒ Deployment Failed: YouTube Blog Converter deployment failed"}' \
                ${{ env.SLACK_WEBHOOK_URL }}
            fi
          fi

      - name: Deployment Summary
        run: |
          echo "## ðŸš€ Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy to DockerHub | ${{ needs.deploy-to-dockerhub.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Container Signing | ${{ needs.sign-container.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy to Azure Green | ${{ needs.deploy-to-azure-green.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validate Green | ${{ needs.validate-green-deployment.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Scan | ${{ needs.owasp-zap-baseline-scan.result == 'success' && 'âœ…' || needs.owasp-zap-baseline-scan.result == 'skipped' && 'â­ï¸' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Test | ${{ needs.jmeter-load-test.result == 'success' && 'âœ…' || needs.jmeter-load-test.result == 'skipped' && 'â­ï¸' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Swap to Production | ${{ needs.swap-to-production.result == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cleanup | ${{ needs.cleanup-green-slot.result == 'success' && 'âœ…' || needs.cleanup-green-slot.result == 'skipped' && 'â­ï¸' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ needs.swap-to-production.result }}" == "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸŽ‰ **Deployment Successful!**" >> $GITHUB_STEP_SUMMARY
            echo "Application URL: [${{ env.PRODUCTION_URL }}](${{ env.PRODUCTION_URL }})" >> $GITHUB_STEP_SUMMARY
          fi