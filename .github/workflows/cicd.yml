name: CI/CD for YouTube Blog Converter

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

# Environment variables
env:
  # Security & Authentication
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  SUPADATA_API_KEY: ${{ secrets.SUPADATA_API_KEY }}
  JWT_SECRET_KEY: ${{ secrets.JWT_SECRET_KEY }}
  JWT_ACCESS_TOKEN_EXPIRES: ${{ secrets.JWT_ACCESS_TOKEN_EXPIRES }}
  MONGODB_URI: ${{ secrets.MONGODB_URI }}
  MONGODB_DB_NAME: ${{ secrets.MONGODB_DB_NAME }}
  FLASK_SECRET_KEY: ${{ secrets.FLASK_SECRET_KEY }}

  # Container Registry
  DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
  CONTAINER_REGISTRY: docker.io
  CONTAINER_IMAGE: youtube-blog-converter

  # SonarQube
  SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
  SONAR_PROJECT_KEY: ${{ vars.SONAR_PROJECT_KEY }}
  SONAR_PROJECT_NAME: ${{ vars.SONAR_PROJECT_NAME }}
  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

  # Application Settings
  FLASK_DEBUG: "false"
  FLASK_HOST: "0.0.0.0"
  OPENAI_MODEL_NAME: ${{ secrets.OPENAI_MODEL_NAME || 'gpt-3.5-turbo' }}
  GA_MEASUREMENT_ID: ${{ secrets.GA_MEASUREMENT_ID }}

  # Monitoring
  PROMETHEUS_ENDPOINT: ${{ secrets.PROMETHEUS_ENDPOINT }}
  GRAFANA_ENDPOINT: ${{ secrets.GRAFANA_ENDPOINT }}
  LOKI_ENDPOINT: ${{ secrets.LOKI_ENDPOINT }}
  MONITORING_ENABLED: "true"
  METRICS_PORT: "8000"

  # Performance Testing
  JMETER_THREADS: ${{ vars.JMETER_THREADS || '50' }}
  JMETER_RAMPUP: ${{ vars.JMETER_RAMPUP || '60' }}
  JMETER_DURATION: ${{ vars.JMETER_DURATION || '300' }}
  JMETER_TARGET_RPS: ${{ vars.JMETER_TARGET_RPS || '100' }}

  # Security Scanning
  TRIVY_SEVERITY: "HIGH,CRITICAL"
  ZAP_SCAN_DURATION: ${{ vars.ZAP_SCAN_DURATION || '10' }}
  ZAP_SPIDER_DURATION: ${{ vars.ZAP_SPIDER_DURATION || '3' }}

permissions:
  issues: write
  contents: read
  pull-requests: write
  checks: write
  id-token: write
  attestations: write

jobs:
  # ===========================
  # STAGE 1: CODE QUALITY
  # ===========================

  lint-and-format:
      name: Lint and Format Check
      runs-on: ubuntu-latest
      steps:
        - name: Checkout code
          uses: actions/checkout@v4
          with:
            fetch-depth: 0

        - name: Set up Python 3.11
          uses: actions/setup-python@v5
          with:
            python-version: 3.11
            cache: 'pip'  # Add caching here too

        - name: Cache pip dependencies
          uses: actions/cache@v4
          with:
            path: ~/.cache/pip
            key: ${{ runner.os }}-pip-lint-${{ hashFiles('**/requirements.txt') }}
            restore-keys: |
              ${{ runner.os }}-pip-lint-
              ${{ runner.os }}-pip-

        - name: Install linting dependencies
          run: |
            python -m pip install --upgrade pip
            pip install flake8 isort

        - name: Run isort (import sorting check)
          continue-on-error: true
          run: |
            echo "::warning::Running import sorting check..."
            if ! isort --check-only --diff app/ tests/; then
              echo "::warning::Import sorting issues found. Consider running 'isort app/ tests/' to fix them."
            else
              echo "✅ Import sorting passed"
            fi

        - name: Run Flake8 (linting)
          continue-on-error: true
          run: |
            echo "::warning::Running code linting check..."
            if ! flake8 app/ tests/ --max-line-length=120 --ignore=E203,W503,W504,F401,F841,F541,E501 --exclude=migrations; then
              echo "::warning::Linting issues found. Please review and fix the code style issues above."
            else
              echo "✅ Linting passed"
            fi

  security-scan:
    name: Security Analysis
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: 3.11
          cache: 'pip'

      - name: Install security scanning tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety pip-audit

      - name: Cache Trivy database
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: ${{ runner.os }}-trivy-db

      - name: Install Trivy
        run: |
          sudo apt-get update -y
          sudo apt-get install -y wget apt-transport-https gnupg lsb-release
          wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main | sudo tee -a /etc/apt/sources.list.d/trivy.list
          sudo apt-get update -y
          sudo apt-get install -y trivy

      - name: Install Gitleaks
        run: |
          wget https://github.com/gitleaks/gitleaks/releases/download/v8.18.2/gitleaks_8.18.2_linux_x64.tar.gz
          tar -xzf gitleaks_8.18.2_linux_x64.tar.gz
          sudo mv gitleaks /usr/local/bin/
          rm gitleaks_8.18.2_linux_x64.tar.gz

      # Run security scans in parallel using background processes
      - name: Run security scans in parallel
        run: |
          # Start all scans in background
          bandit -r app/ -f json -o bandit-report.json &
          BANDIT_PID=$!
          
          safety check --json --output safety-report.json &
          SAFETY_PID=$!
          
          pip-audit --format=json --output=dependency-audit.json &
          AUDIT_PID=$!
          
          trivy fs --severity ${{ env.TRIVY_SEVERITY || 'HIGH,CRITICAL' }} --exit-code 0 --format sarif -o trivy-fs-report.sarif . &
          TRIVY_PID=$!
          
          gitleaks detect --source . --report-format json --report-path gitleaks-report.json --commit ${{ github.sha }} --exit-code 0 &
          GITLEAKS_PID=$!
          
          # Wait for all to complete
          wait $BANDIT_PID || echo "Bandit completed with warnings"
          wait $SAFETY_PID || echo "Safety completed with warnings"
          wait $AUDIT_PID || echo "Audit completed with warnings"
          wait $TRIVY_PID || echo "Trivy completed"
          wait $GITLEAKS_PID || echo "Gitleaks completed"

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            dependency-audit.json
            trivy-fs-report.sarif
            gitleaks-report.json
          retention-days: 30

  build-and-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Add timeout to prevent hanging jobs
    steps:
      - name: Show commit author
        run: |
          echo "Commit by ${{ github.actor }}"
          echo "Email: ${{ github.event.head_commit.author.email || 'N/A' }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist

      # Fix the Loki connection issue by disabling external logging during tests
      - name: Run tests with coverage + JUnit (optimized)
        env:
          TESTING: true
          FLASK_ENV: testing
          CI: true
          LOG_TO_FILE: false
          LOG_LEVEL: ERROR  # Reduce log verbosity
          DISABLE_EXTERNAL_LOGGING: true  # Add this to disable Loki connections
        run: |
          # Run tests in parallel using pytest-xdist
          pytest --cov=app \
                --cov-report=xml:coverage.xml \
                --cov-report=html:htmlcov \
                --junitxml=pytest-results.xml \
                -v tests/ \
                -n auto \
                --maxfail=5 \
                --tb=short

          # Ensure coverage.xml exists
          if [ ! -f coverage.xml ]; then
            echo "<?xml version='1.0' encoding='UTF-8'?><coverage version='1'><sources></sources><packages></packages></coverage>" > coverage.xml
          fi

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            coverage.xml
            htmlcov/
          retention-days: 30

      - name: Upload pytest results
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: pytest-results.xml
          retention-days: 30

  generate-sbom:
    name: Generate Software Bill of Materials
    runs-on: ubuntu-latest
    needs: [build-and-test]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: 3.11

      - name: Install dependencies for SBOM generation
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install cyclonedx-bom

      - name: Generate JSON SBOM from requirements
        run: |
          cyclonedx-py requirements -i requirements.txt -o sbom.json --output-format json

      - name: Generate XML SBOM from requirements
        run: |
          cyclonedx-py requirements -i requirements.txt -o sbom.xml --output-format xml

      - name: Validate SBOM files
        run: |
          echo "Validating SBOM files..."
          ls -la sbom.*
          echo "JSON SBOM content type:"
          file sbom.json
          echo "XML SBOM content type:"
          file sbom.xml

      - name: Upload SBOM artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sbom-reports
          path: |
            sbom.json
            sbom.xml

  sonarqube-scan:
      runs-on: self-hosted
      needs: build-and-test
      steps:
        - name: Show commit author
          run: |
            echo "Commit by ${{ github.actor }}"
            echo "Email: ${{ github.event.head_commit.author.email || 'N/A' }}"

        - name: Checkout code
          uses: actions/checkout@v4
          with:
            fetch-depth: 0

        - name: Download coverage report
          uses: actions/download-artifact@v4
          with:
            name: coverage-report
            path: .

        - name: Download security reports
          uses: actions/download-artifact@v4
          with:
            name: security-reports
            path: .

        - name: SonarQube Scan with Quality Gate
          id: sonar-scan
          uses: SonarSource/sonarqube-scan-action@v5.3.0
          env:
            SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
            SONAR_HOST_URL: ${{ env.SONAR_HOST_URL }}
          with:
            args: >
              -Dsonar.python.coverage.reportPaths=coverage.xml
              -Dsonar.python.version=3.12
              -Dsonar.sources=app
              -Dsonar.tests=tests
              -Dsonar.test.inclusions=tests/**
              -Dsonar.exclusions=**/__pycache__/**,**/.pytest_cache/**,**/env/**
              -Dsonar.projectKey=${{ env.SONAR_PROJECT_KEY }}
              -Dsonar.projectName=${{ env.SONAR_PROJECT_NAME }}
              -Dsonar.projectVersion=1.0.${{ github.run_number }}
              -Dsonar.qualitygate.wait=true
              -Dsonar.scm.provider=git
              -Dsonar.links.scm=${{ github.server_url }}/${{ github.repository }}
              -Dsonar.links.ci=${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
              -Dsonar.python.bandit.reportPaths=bandit-report.json

        - name: Create Detailed SonarQube PR Comment
          if: github.event_name == 'pull_request'
          uses: actions/github-script@v7
          with:
            script: |
              const prNumber = context.issue.number;
              const comment = `## 📊 SonarQube Code Quality Report

              🔍 **Analysis completed for PR #${prNumber}**

              ### Quality Gate Status
              - **Overall Status:** Analysis completed
              - **Project:** ${{ env.SONAR_PROJECT_NAME }}
              - **Analysis Date:** ${new Date().toISOString()}

              ### Key Metrics
              - **Coverage:** Linked in SonarQube dashboard
              - **Duplications:** Check SonarQube dashboard
              - **Maintainability:** Review code smells
              - **Reliability:** Check bug count
              - **Security:** Review security hotspots

              👉 View full report: ${{ env.SONAR_HOST_URL }}/dashboard?id=${{ env.SONAR_PROJECT_KEY }}

              *This comment was automatically generated by CI/CD.*`;

              github.rest.issues.createComment({
                issue_number: prNumber,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });

  # ===========================
  # STAGE 3: CONTAINER BUILD & SECURITY
  # ===========================

  docker-build:
    name: Build Docker Image & Security Scan
    needs: [security-scan, build-and-test, sonarqube-scan, generate-sbom]
    runs-on: ubuntu-latest
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Show commit author
        run: |
          echo "Commit by ${{ github.actor }}"
          echo "Email: ${{ github.event.head_commit.author.email || 'N/A' }}"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install Trivy
        run: |
          sudo apt-get update -y
          sudo apt-get install -y wget apt-transport-https gnupg lsb-release
          sudo mkdir -p /etc/apt/keyrings
          curl -fsSL https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo gpg --dearmor -o /etc/apt/keyrings/trivy.gpg
          echo "deb [signed-by=/etc/apt/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee /etc/apt/sources.list.d/trivy.list
          sudo apt-get update -y
          sudo apt-get install -y trivy

      - name: Build Docker image
        id: build
        run: |
          docker build -t ${{ env.CONTAINER_IMAGE }}:${{ github.sha }} .
          digest=$(docker inspect --format='{{index .RepoDigests 0}}' ${{ env.CONTAINER_IMAGE }}:${{ github.sha }} 2>/dev/null || true)
          if [ -z "$digest" ]; then
            digest="none"
          fi
          echo "digest=$digest" >> $GITHUB_OUTPUT

      - name: Container Vulnerability Scan
        run: trivy image --severity ${{ env.TRIVY_SEVERITY }} --exit-code 0 --format sarif -o trivy-container-report.sarif ${{ env.CONTAINER_IMAGE }}:${{ github.sha }}

      - name: Upload container security report
        uses: actions/upload-artifact@v4
        with:
          name: trivy-container-report
          path: trivy-container-report.sarif
          retention-days: 30

      - name: Run smoke test
        run: |
          # Run container
          docker run -d --name smoke-test \
            -e FLASK_DEBUG="${{ env.FLASK_DEBUG }}" \
            -e FLASK_HOST="${{ env.FLASK_HOST }}" \
            -p 5000:5000 \
            ${{ env.CONTAINER_IMAGE }}:${{ github.sha }} || true

          echo "⏳ Waiting for Flask app to start..."
          for i in {1..10}; do
            if docker ps -a | grep -q smoke-test && docker logs smoke-test 2>&1 | grep -q "Running on"; then
              echo "Flask started ✅"
              break
            fi
            echo "Retry $i..."
            sleep 2
          done

          echo "📜 Container logs:"
          docker logs smoke-test || echo "No logs available"

          echo "🌐 Running health check..."
          if curl -s http://localhost:5000/health > /dev/null; then
            echo "Health check passed ✅"
          else
            echo "⚠️ Health check failed, but continuing..."
          fi

          # Stop and remove container
          docker stop smoke-test || true
          docker rm smoke-test || true

  # ===========================
  # STAGE 4: DEPLOYMENT
  # ===========================

  deploy-to-dockerhub:
      needs: [docker-build]
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      runs-on: ubuntu-latest
      outputs:
        image-digest: ${{ steps.build.outputs.digest }}

      steps:
        - name: Checkout code
          uses: actions/checkout@v4

        - name: Set up Docker Buildx
          uses: docker/setup-buildx-action@v3

        - name: Login to Docker Hub
          uses: docker/login-action@v3
          with:
            username: ${{ env.DOCKERHUB_USERNAME }}
            password: ${{ secrets.DOCKERHUB_TOKEN }}

        - name: Build and push
          id: build
          uses: docker/build-push-action@v5
          with:
            context: .
            push: true
            tags: |
              ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:latest
              ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ github.sha }}
            cache-from: type=gha
            cache-to: type=gha,mode=max

        - name: Create issue on failure
          if: ${{ failure() }}
          uses: actions/github-script@v7
          with:
            script: |
              github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: '❌ DockerHub Deployment Failed',
                body: 'Docker image push to DockerHub failed.\n\n**Details:**\n- Workflow: ${{ github.workflow }}\n- Run: ${{ github.run_number }}\n- Branch: ${{ github.ref_name }}\n- Commit: ${{ github.sha }}\n\nPlease check DockerHub credentials and connectivity.',
                labels: ['bug', 'deployment-failure', 'docker']
              });

        - name: Slack Notification on Failure
          if: ${{ failure() }}
          uses: act10ns/slack@v2.1.0
          with:
            status: ${{ job.status }}
            webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
            message: |
              🚨 **DockerHub Deployment Failed** 🚨

              **Branch:** ${{ github.ref_name }}
              **Commit:** ${{ github.sha }}

              Check DockerHub credentials and connectivity.
              Logs: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

  sign-container:
    name: Sign Container Images
    runs-on: ubuntu-latest
    needs: [deploy-to-dockerhub]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Install Cosign
        uses: sigstore/cosign-installer@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ env.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Sign container image
        run: |
          cosign sign --yes ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ github.sha }}
          echo "SIGNING_FAILED=false" >> $GITHUB_ENV
        env:
          COSIGN_EXPERIMENTAL: 1
        continue-on-error: true

      - name: Handle signing failure
        if: failure()
        run: |
          echo "SIGNING_FAILED=true" >> $GITHUB_ENV
          echo "⚠️ Container signing failed, but continuing deployment"

      - name: Download SBOM artifacts
        if: env.SIGNING_FAILED != 'true'
        uses: actions/download-artifact@v4
        with:
          name: sbom-reports
          path: ./sbom/

      - name: Generate and attach SBOM
        if: env.SIGNING_FAILED != 'true'
        run: |
          echo "📄 Attaching SBOM to signed container..."
          if [ -f ./sbom/sbom.json ]; then
            cosign attest --yes --predicate ./sbom/sbom.json ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ github.sha }}
            echo "SBOM attached successfully"
          else
            echo "⚠️ SBOM file not found, skipping attestation"
          fi
        env:
          COSIGN_EXPERIMENTAL: 1

      - name: Verify container signature
        if: env.SIGNING_FAILED != 'true'
        run: |
          cosign verify \
            --certificate-identity-regexp="https://github.com/${{ github.repository }}/*" \
            --certificate-oidc-issuer=https://token.actions.githubusercontent.com \
            ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ github.sha }}
        env:
          COSIGN_EXPERIMENTAL: 1

  # ===========================
  # AZURE BLUE-GREEN DEPLOYMENT
  # ===========================

  deploy-to-azure-green:
    name: Deploy to Azure Green Slot
    needs: [deploy-to-dockerhub, sign-container]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment:
      name: azure-green  # Must match the federated credential environment name
      url: https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net
    
    # Add explicit permissions for this job
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to Azure with OIDC
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Ensure Green Slot is Started
        run: |
          echo "🔄 Ensuring Green slot is started before deployment..."

          # Check current status of green slot
          GREEN_STATUS=$(az webapp show \
            --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
            --name ${{ secrets.AZURE_WEBAPP_NAME }} \
            --slot green \
            --query "state" \
            --output tsv 2>/dev/null || echo "Unknown")

          echo "Current Green slot status: $GREEN_STATUS"

          if [ "$GREEN_STATUS" = "Stopped" ]; then
            echo "⚡ Green slot is stopped - starting it now..."
            az webapp start \
              --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
              --name ${{ secrets.AZURE_WEBAPP_NAME }} \
              --slot green

            echo "⏳ Waiting for Green slot to start..."
            sleep 30

            # Verify it started
            NEW_STATUS=$(az webapp show \
              --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
              --name ${{ secrets.AZURE_WEBAPP_NAME }} \
              --slot green \
              --query "state" \
              --output tsv)

            echo "✅ Green slot status after start: $NEW_STATUS"
          elif [ "$GREEN_STATUS" = "Running" ]; then
            echo "✅ Green slot is already running"
          else
            echo "ℹ️ Green slot status: $GREEN_STATUS - proceeding with deployment"
          fi

      - name: Deploy to Green Slot
        uses: azure/webapps-deploy@v3
        with:
          app-name: ${{ secrets.AZURE_WEBAPP_NAME }}
          slot-name: green
          images: ${{ env.DOCKERHUB_USERNAME }}/${{ env.CONTAINER_IMAGE }}:${{ github.sha }}

      - name: Wait for Green Slot to be ready
        run: |
          echo "⏳ Waiting for Green slot to be ready..."
          for i in {1..30}; do
            response=$(curl -s -o /dev/null -w "%{http_code}" https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net/health || echo "000")
            if [ "$response" = "200" ]; then
              echo "✅ Green slot is healthy"
              break
            fi
            echo "Attempt $i/30: Health check returned $response"
            sleep 10
          done

      - name: Run smoke tests on Green slot
        id: smoke-tests
        run: |
          GREEN_URL="https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net"

          # Test health endpoint
          echo "Testing health endpoint..."
          health_response=$(curl -s -o /dev/null -w "%{http_code}" $GREEN_URL/health)
          if [ "$health_response" != "200" ]; then
            echo "❌ Health check failed with status $health_response"
            exit 1
          fi
          echo "✅ Health check passed"

          # Test main page
          echo "Testing main page..."
          main_response=$(curl -s -o /dev/null -w "%{http_code}" $GREEN_URL/)
          if [ "$main_response" != "200" ]; then
            echo "⚠️ Main page returned $main_response"
          fi
          echo "✅ Main page accessible"

          # Additional API tests can be added here
          echo "All smoke tests passed successfully"

      - name: Slack Notification - Green Deployment Success
        if: success()
        uses: act10ns/slack@v2.1.0
        with:
          status: ${{ job.status }}
          webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
          message: |
            ✅ **Green Slot Deployment Successful**

            **Version:** ${{ github.sha }}
            **Green URL:** https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net

            Ready for swap approval to production.
            Please verify the green environment before approving production swap.

  validate-green-deployment:
    name: Validate Green Deployment
    needs: [deploy-to-azure-green]
    runs-on: ubuntu-latest
    environment:
      name: azure-green-validation

    steps:
      - name: Performance Test on Green Slot
        run: |
          GREEN_URL="https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net"

          echo "🔍 Running performance validation on Green slot..."

          # Simple load test (you can replace with more sophisticated testing)
          for i in {1..10}; do
            response_time=$(curl -o /dev/null -s -w '%{time_total}\n' $GREEN_URL/health)
            echo "Request $i: ${response_time}s"

            # Check if bc is available, if not skip the comparison
            if command -v bc >/dev/null 2>&1; then
              if (( $(echo "$response_time > 2" | bc -l) )); then
                echo "⚠️ Response time exceeds threshold: ${response_time}s"
              fi
            else
              echo "Response time: ${response_time}s (threshold check skipped)"
            fi
          done

          echo "✅ Performance validation completed"

      - name: Security Headers Check
        run: |
          GREEN_URL="https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net"

          echo "🔒 Checking security headers..."
          headers=$(curl -sI $GREEN_URL)

          # Check for important security headers
          if echo "$headers" | grep -qi "X-Content-Type-Options"; then
            echo "✅ X-Content-Type-Options header present"
          else
            echo "⚠️ Missing X-Content-Type-Options header"
          fi

          if echo "$headers" | grep -qi "X-Frame-Options"; then
            echo "✅ X-Frame-Options header present"
          else
            echo "⚠️ Missing X-Frame-Options header"
          fi

  owasp-zap-baseline-scan:
    name: OWASP ZAP Security Scan
    runs-on: ubuntu-latest
    needs: validate-green-deployment
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Get Green Environment URL
        run: |
          echo "Using Green Environment for OWASP ZAP scan"
          echo "GREEN_URL=https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net" >> $GITHUB_ENV

      - name: Wait for Green Environment
        run: |
          echo "⏳ Waiting for green environment to be fully ready for security scanning..."
          sleep 120

          # Verify the green environment is accessible
          if curl -f -s "${{ env.GREEN_URL }}/health" > /dev/null; then
            echo "✅ Green environment is ready for security scanning"
          else
            echo "⚠️ Green environment health check failed - proceeding with scan anyway"
          fi

      - name: ZAP Baseline Scan
        run: |
          echo "🔍 Starting OWASP ZAP Baseline Security Scan..."

          # Create ZAP configuration
          cat > zap-config.conf << 'EOF'
          # ZAP Configuration for YouTube Blog Converter

          # Rules to disable (adjust based on your application)
          rules.disable.10020=false  # Missing Anti-clickjacking Header
          rules.disable.10021=false  # X-Content-Type-Options header missing
          rules.disable.10023=false  # Information Disclosure - Debug Error Messages

          # Authentication configuration (if needed)
          # auth.method=form
          # auth.loginurl=/login
          # auth.username=testuser
          # auth.password=testpass
          EOF

          # Run ZAP Docker container for baseline scan
          docker run --rm \
            -v $(pwd):/zap/wrk/ \
            -t ghcr.io/zaproxy/zaproxy:stable \
            zap-baseline.py \
            -t "${{ env.GREEN_URL }}" \
            -d \
            -m ${{ env.ZAP_SCAN_DURATION }} \
            -z "-config api.disablekey=true" \
            -J zap-baseline-report.json \
            -w zap-baseline-report.md \
            -r zap-baseline-report.html

      - name: Parse ZAP Scan Results
        id: parse-results
        if: always()
        run: |
          echo "📊 Parsing ZAP scan results..."

          if [ -f "zap-baseline-report.json" ]; then
            # Extract key metrics
            HIGH_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("High"))] | length' zap-baseline-report.json || echo "0")
            MEDIUM_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Medium"))] | length' zap-baseline-report.json || echo "0")
            LOW_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Low"))] | length' zap-baseline-report.json || echo "0")
            INFO_ALERTS=$(jq '[.site[].alerts[] | select(.riskdesc | startswith("Informational"))] | length' zap-baseline-report.json || echo "0")

            echo "high_alerts=$HIGH_ALERTS" >> $GITHUB_OUTPUT
            echo "medium_alerts=$MEDIUM_ALERTS" >> $GITHUB_OUTPUT
            echo "low_alerts=$LOW_ALERTS" >> $GITHUB_OUTPUT
            echo "info_alerts=$INFO_ALERTS" >> $GITHUB_OUTPUT

            echo "🔍 ZAP Scan Results Summary:"
            echo "  - High Risk: $HIGH_ALERTS"
            echo "  - Medium Risk: $MEDIUM_ALERTS"
            echo "  - Low Risk: $LOW_ALERTS"
            echo "  - Informational: $INFO_ALERTS"

            # Set security gate status
            if [ "$HIGH_ALERTS" -eq 0 ] && [ "$MEDIUM_ALERTS" -le 5 ]; then
              echo "security_gate_passed=true" >> $GITHUB_OUTPUT
              echo "✅ Security gate passed"
            else
              echo "security_gate_passed=false" >> $GITHUB_OUTPUT
              echo "❌ Security gate failed"
            fi
          else
            echo "⚠️ ZAP report file not found"
            echo "high_alerts=unknown" >> $GITHUB_OUTPUT
            echo "medium_alerts=unknown" >> $GITHUB_OUTPUT
            echo "low_alerts=unknown" >> $GITHUB_OUTPUT
            echo "info_alerts=unknown" >> $GITHUB_OUTPUT
            echo "security_gate_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Security Summary Report
        id: generate-summary
        if: always()
        run: |
          cat > security-summary.md << 'EOF'
          # 🔒 OWASP ZAP Security Scan Report

          ## Scan Details
          - **Target:** ${{ env.GREEN_URL }}
          - **Scan Type:** Baseline Security Scan
          - **Duration:** ${{ env.ZAP_SCAN_DURATION }} minutes
          - **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          - **Branch:** ${{ github.ref_name }}
          - **Commit:** ${{ github.sha }}

          ## Results Summary
          - **High Risk:** ${{ steps.parse-results.outputs.high_alerts }} alerts
          - **Medium Risk:** ${{ steps.parse-results.outputs.medium_alerts }} alerts
          - **Low Risk:** ${{ steps.parse-results.outputs.low_alerts }} alerts
          - **Informational:** ${{ steps.parse-results.outputs.info_alerts }} alerts

          ## Security Gate Status
          **Status:** ${{ steps.parse-results.outputs.security_gate_passed == 'true' && '✅ PASSED' || '❌ FAILED' }}

          ### Gate Criteria
          - High Risk alerts: 0 (current: ${{ steps.parse-results.outputs.high_alerts }})
          - Medium Risk alerts: ≤ 5 (current: ${{ steps.parse-results.outputs.medium_alerts }})

          ## Recommendations
          ${{ steps.parse-results.outputs.security_gate_passed == 'true' &&
             '- Continue with deployment process
             - Monitor security alerts in production
             - Schedule regular security scans' ||
             '- Review and fix high/medium risk vulnerabilities
             - Re-run security scan before deployment
             - Consider implementing additional security controls' }}

          ## Files Generated
          - **JSON Report:** `zap-baseline-report.json`
          - **HTML Report:** `zap-baseline-report.html`
          - **Markdown Report:** `zap-baseline-report.md`

          *This report was automatically generated by the CI/CD pipeline.*
          EOF

          cat security-summary.md

      - name: Upload ZAP Scan Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: owasp-zap-reports
          path: |
            zap-baseline-report.json
            zap-baseline-report.html
            zap-baseline-report.md
            security-summary.md
          retention-days: 30

      - name: Security Gate Evaluation
        id: security-gate
        if: always()
        run: |
          if [ "${{ steps.parse-results.outputs.security_gate_passed }}" = "true" ]; then
            echo "✅ Security gate passed - proceeding with deployment"
            echo "gate_passed=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Security gate failed - blocking deployment"
            echo "gate_passed=false" >> $GITHUB_OUTPUT

            echo "🚨 Critical security issues detected:"
            echo "  - High Risk Alerts: ${{ steps.parse-results.outputs.high_alerts }}"
            echo "  - Medium Risk Alerts: ${{ steps.parse-results.outputs.medium_alerts }}"

            if [ "${{ steps.parse-results.outputs.high_alerts }}" != "0" ]; then
              echo "❌ CRITICAL: High risk vulnerabilities must be fixed before deployment"
              exit 1
            fi

            if [ "${{ steps.parse-results.outputs.medium_alerts }}" -gt 5 ]; then
              echo "⚠️ WARNING: Too many medium risk vulnerabilities (limit: 5, found: ${{ steps.parse-results.outputs.medium_alerts }})"
              exit 1
            fi
          fi

      - name: Create Detailed Security Issue (on failure)
        if: failure() && steps.security-gate.outputs.gate_passed == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let securitySummary = 'Security scan completed with issues.';

            try {
              securitySummary = fs.readFileSync('security-summary.md', 'utf8');
            } catch (error) {
              console.log('Could not read security summary file');
            }

            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🔒 Critical Security Vulnerabilities Detected',
              body: `**SECURITY GATE FAILED** - Deployment blocked due to critical security issues.

            ${securitySummary}

            ## Immediate Actions Required
            1. Review detailed security reports in the workflow artifacts
            2. Fix all high-risk vulnerabilities
            3. Reduce medium-risk vulnerabilities to 5 or fewer
            4. Re-run the security scan to verify fixes

            ## Workflow Details
            - **Workflow:** ${{ github.workflow }}
            - **Run:** ${{ github.run_number }}
            - **Branch:** ${{ github.ref_name }}
            - **Commit:** ${{ github.sha }}
            - **Target:** ${{ env.GREEN_URL }}

            **⚠️ Production deployment is blocked until these security issues are resolved.**`,
              labels: ['security', 'critical', 'blocked-deployment']
            });

      - name: Slack Critical Security Alert
        if: failure() && steps.security-gate.outputs.gate_passed == 'false'
        uses: act10ns/slack@v2.1.0
        with:
          status: failure
          webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
          message: |
            🚨 **CRITICAL SECURITY ALERT** 🚨

            **DEPLOYMENT BLOCKED** - Security gate failed

            **High Risk:** ${{ steps.parse-results.outputs.high_alerts }} alerts
            **Medium Risk:** ${{ steps.parse-results.outputs.medium_alerts }} alerts

            **Target:** ${{ env.GREEN_URL }}
            **Branch:** ${{ github.ref_name }}
            **Commit:** ${{ github.sha }}

            **Action Required:** Fix security vulnerabilities before deployment

            📊 View detailed reports: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

      - name: Slack Success Notification
        if: success()
        uses: act10ns/slack@v2.1.0
        with:
          status: success
          webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
          message: |
            ✅ **Security Scan Passed** ✅

            **OWASP ZAP Baseline Scan Completed**

            **Results:**
            - High Risk: ${{ steps.parse-results.outputs.high_alerts }}
            - Medium Risk: ${{ steps.parse-results.outputs.medium_alerts }}
            - Low Risk: ${{ steps.parse-results.outputs.low_alerts }}

            **Target:** ${{ env.GREEN_URL }}
            **Security Gate:** ✅ PASSED

            Proceeding with deployment pipeline...

  jmeter-load-test:
    name: Performance Load Testing
    runs-on: ubuntu-latest
    needs: [owasp-zap-baseline-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 45

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JMeter
        run: |
          echo "📥 Downloading and setting up Apache JMeter..."
          wget -q https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.5.tgz
          tar -xzf apache-jmeter-5.5.tgz
          echo "JMETER_HOME=$(pwd)/apache-jmeter-5.5" >> $GITHUB_ENV
          echo "$(pwd)/apache-jmeter-5.5/bin" >> $GITHUB_PATH

      - name: Get Green Environment URL
        run: |
          echo "Using Green Environment for load testing"
          echo "GREEN_URL=https://yt-agent-green-gta9gtdcfkh5cha6.centralus-01.azurewebsites.net" >> $GITHUB_ENV

      - name: Create JMeter Test Plan
        run: |
          echo "📝 Creating comprehensive JMeter test plan..."

          cat > youtube-blog-converter-load-test.jmx << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <jmeterTestPlan version="1.2" properties="5.0" jmeter="5.5">
            <hashTree>
              <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="YouTube Blog Converter Load Test" enabled="true">
                <stringProp name="TestPlan.comments">Comprehensive load test for YouTube Blog Converter application</stringProp>
                <boolProp name="TestPlan.functional_mode">false</boolProp>
                <boolProp name="TestPlan.tearDown_on_shutdown">true</boolProp>
                <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
                <elementProp name="TestPlan.arguments" elementType="Arguments" guiclass="ArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                  <collectionProp name="Arguments.arguments">
                    <elementProp name="BASE_URL" elementType="Argument">
                      <stringProp name="Argument.name">BASE_URL</stringProp>
                      <stringProp name="Argument.value">${{ env.GREEN_URL }}</stringProp>
                      <stringProp name="Argument.metadata">=</stringProp>
                    </elementProp>
                  </collectionProp>
                </elementProp>
              </TestPlan>
              <hashTree>
                <!-- Thread Group for Load Testing -->
                <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Load Test Users" enabled="true">
                  <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
                  <elementProp name="ThreadGroup.main_controller" elementType="LoopController" guiclass="LoopControllerGui" testclass="LoopController" testname="Loop Controller" enabled="true">
                    <boolProp name="LoopController.continue_forever">false</boolProp>
                    <intProp name="LoopController.loops">10</intProp>
                  </elementProp>
                  <stringProp name="ThreadGroup.num_threads">${{ env.JMETER_THREADS }}</stringProp>
                  <stringProp name="ThreadGroup.ramp_time">${{ env.JMETER_RAMPUP }}</stringProp>
                  <boolProp name="ThreadGroup.scheduler">true</boolProp>
                  <stringProp name="ThreadGroup.duration">${{ env.JMETER_DURATION }}</stringProp>
                  <stringProp name="ThreadGroup.delay">0</stringProp>
                </ThreadGroup>
                <hashTree>
                  <!-- HTTP Request Defaults -->
                  <ConfigTestElement guiclass="HttpDefaultsGui" testclass="ConfigTestElement" testname="HTTP Request Defaults" enabled="true">
                    <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="HTTPArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                      <collectionProp name="Arguments.arguments"/>
                    </elementProp>
                    <stringProp name="HTTPSampler.domain">${{ env.GREEN_URL }}</stringProp>
                    <stringProp name="HTTPSampler.port"></stringProp>
                    <stringProp name="HTTPSampler.protocol">https</stringProp>
                    <stringProp name="HTTPSampler.contentEncoding"></stringProp>
                    <stringProp name="HTTPSampler.path"></stringProp>
                  </ConfigTestElement>
                  <hashTree/>

                  <!-- Health Check Endpoint -->
                  <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="01 - Health Check" enabled="true">
                    <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="HTTPArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                      <collectionProp name="Arguments.arguments"/>
                    </elementProp>
                    <stringProp name="HTTPSampler.path">/health</stringProp>
                    <stringProp name="HTTPSampler.method">GET</stringProp>
                    <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                    <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                    <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                  </HTTPSamplerProxy>
                  <hashTree>
                    <!-- Response Assertion for Health Check -->
                    <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Health Check Response Assertion" enabled="true">
                      <collectionProp name="Asserion.test_strings">
                        <stringProp name="49586">200</stringProp>
                      </collectionProp>
                      <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                      <boolProp name="Assertion.assume_success">false</boolProp>
                      <intProp name="Assertion.test_type">1</intProp>
                    </ResponseAssertion>
                    <hashTree/>
                  </hashTree>

                  <!-- Home Page -->
                  <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="02 - Home Page" enabled="true">
                    <elementProp name="HTTPsampler.Arguments" elementType="Arguments" guiclass="HTTPArgumentsPanel" testclass="Arguments" testname="User Defined Variables" enabled="true">
                      <collectionProp name="Arguments.arguments"/>
                    </elementProp>
                    <stringProp name="HTTPSampler.path">/</stringProp>
                    <stringProp name="HTTPSampler.method">GET</stringProp>
                    <boolProp name="HTTPSampler.follow_redirects">true</boolProp>
                    <boolProp name="HTTPSampler.auto_redirects">false</boolProp>
                    <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
                  </HTTPSamplerProxy>
                  <hashTree>
                    <!-- Response Assertion for Home Page -->
                    <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Home Page Response Assertion" enabled="true">
                      <collectionProp name="Asserion.test_strings">
                        <stringProp name="49586">200</stringProp>
                      </collectionProp>
                      <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                      <boolProp name="Assertion.assume_success">false</boolProp>
                      <intProp name="Assertion.test_type">1</intProp>
                    </ResponseAssertion>
                    <hashTree/>
                  </hashTree>

                  <!-- Think Time -->
                  <UniformRandomTimer guiclass="UniformRandomTimerGui" testclass="UniformRandomTimer" testname="Think Time" enabled="true">
                    <stringProp name="ConstantTimer.delay">1000</stringProp>
                    <stringProp name="RandomTimer.range">2000</stringProp>
                  </UniformRandomTimer>
                  <hashTree/>
                </hashTree>
              </hashTree>
            </hashTree>
          </jmeterTestPlan>
          EOF

      - name: Verify Green Environment Accessibility
        id: verify-environment
        run: |
          echo "🔍 Verifying green environment is accessible for load testing..."

          MAX_ATTEMPTS=5
          ATTEMPT=1

          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "Verification attempt $ATTEMPT/$MAX_ATTEMPTS..."

            if curl -f -s "${{ env.GREEN_URL }}/health" > /dev/null; then
              echo "✅ Green environment is accessible"
              echo "environment_ready=true" >> $GITHUB_OUTPUT
              break
            else
              echo "❌ Green environment not accessible (attempt $ATTEMPT/$MAX_ATTEMPTS)"

              if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
                echo "❌ Green environment is not ready for load testing"
                echo "environment_ready=false" >> $GITHUB_OUTPUT
                exit 1
              fi
            fi

            ATTEMPT=$((ATTEMPT + 1))
            sleep 30
          done

      - name: Run JMeter Load Test
        id: run-jmeter
        run: |
          echo "🚀 Starting JMeter load test..."
          echo "Target: ${{ env.GREEN_URL }}"
          echo "Threads: ${{ env.JMETER_THREADS }}"
          echo "Ramp-up: ${{ env.JMETER_RAMPUP }}s"
          echo "Duration: ${{ env.JMETER_DURATION }}s"

          # Create results directory
          mkdir -p jmeter-results

          # Run JMeter test
          if jmeter -n \
            -t youtube-blog-converter-load-test.jmx \
            -l jmeter-results/results.jtl \
            -e -o jmeter-results/html-report \
            -Jthreads=${{ env.JMETER_THREADS }} \
            -Jrampup=${{ env.JMETER_RAMPUP }} \
            -Jduration=${{ env.JMETER_DURATION }}; then
            echo "✅ JMeter test completed successfully"
            echo "jmeter_success=true" >> $GITHUB_OUTPUT
          else
            echo "❌ JMeter test failed"
            echo "jmeter_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Analyze Load Test Results
        id: analyze-results
        if: steps.run-jmeter.outputs.jmeter_success == 'true'
        run: |
          echo "📊 Analyzing load test results..."

          if [ -f "jmeter-results/results.jtl" ]; then
            # Extract key performance metrics using awk
            TOTAL_SAMPLES=$(awk -F',' 'END{print NR-1}' jmeter-results/results.jtl)
            ERRORS=$(awk -F',' '$8=="false" {count++} END{print count+0}' jmeter-results/results.jtl)
            ERROR_RATE=$(awk -v errors=$ERRORS -v total=$TOTAL_SAMPLES 'BEGIN{printf "%.2f", (errors/total)*100}')

            # Calculate response time statistics
            AVG_RESPONSE_TIME=$(awk -F',' '{sum+=$2; count++} END{printf "%.0f", sum/count}' jmeter-results/results.jtl | tail -n +2)
            P95_RESPONSE_TIME=$(awk -F',' '{print $2}' jmeter-results/results.jtl | tail -n +2 | sort -n | awk '{all[NR] = $0} END{print all[int(NR*0.95 - 0.5)]}')
            P99_RESPONSE_TIME=$(awk -F',' '{print $2}' jmeter-results/results.jtl | tail -n +2 | sort -n | awk '{all[NR] = $0} END{print all[int(NR*0.99 - 0.5)]}')

            # Calculate throughput (requests per second)
            DURATION_SECONDS=${{ env.JMETER_DURATION }}
            THROUGHPUT=$(awk -v total=$TOTAL_SAMPLES -v duration=$DURATION_SECONDS 'BEGIN{printf "%.2f", total/duration}')

            echo "📈 Performance Test Results:"
            echo "  - Total Samples: $TOTAL_SAMPLES"
            echo "  - Errors: $ERRORS"
            echo "  - Error Rate: $ERROR_RATE%"
            echo "  - Average Response Time: ${AVG_RESPONSE_TIME}ms"
            echo "  - 95th Percentile: ${P95_RESPONSE_TIME}ms"
            echo "  - 99th Percentile: ${P99_RESPONSE_TIME}ms"
            echo "  - Throughput: $THROUGHPUT req/sec"

            # Set outputs
            echo "total_samples=$TOTAL_SAMPLES" >> $GITHUB_OUTPUT
            echo "errors=$ERRORS" >> $GITHUB_OUTPUT
            echo "error_rate=$ERROR_RATE" >> $GITHUB_OUTPUT
            echo "avg_response_time=$AVG_RESPONSE_TIME" >> $GITHUB_OUTPUT
            echo "p95_response_time=$P95_RESPONSE_TIME" >> $GITHUB_OUTPUT
            echo "p99_response_time=$P99_RESPONSE_TIME" >> $GITHUB_OUTPUT
            echo "throughput=$THROUGHPUT" >> $GITHUB_OUTPUT

            # Performance gate evaluation
            if (( $(echo "$ERROR_RATE < 1.0" | bc -l) )) && \
               (( $(echo "$AVG_RESPONSE_TIME < 2000" | bc -l) )) && \
               (( $(echo "$P95_RESPONSE_TIME < 5000" | bc -l) )); then
              echo "performance_gate_passed=true" >> $GITHUB_OUTPUT
              echo "✅ Performance gate passed"
            else
              echo "performance_gate_passed=false" >> $GITHUB_OUTPUT
              echo "❌ Performance gate failed"
              echo "  - Error rate threshold: < 1.0% (actual: $ERROR_RATE%)"
              echo "  - Avg response time threshold: < 2000ms (actual: ${AVG_RESPONSE_TIME}ms)"
              echo "  - P95 response time threshold: < 5000ms (actual: ${P95_RESPONSE_TIME}ms)"
            fi
          else
            echo "❌ JMeter results file not found"
            echo "performance_gate_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Handle JMeter Failure
        if: steps.run-jmeter.outputs.jmeter_success != 'true'
        run: |
          echo "❌ JMeter load test execution failed"

          # Check for JMeter logs
          if [ -f "jmeter.log" ]; then
            echo "📋 JMeter Error Log:"
            tail -50 jmeter.log
          fi

          # Set failure outputs
          echo "total_samples=0" >> $GITHUB_OUTPUT
          echo "errors=unknown" >> $GITHUB_OUTPUT
          echo "error_rate=unknown" >> $GITHUB_OUTPUT
          echo "avg_response_time=unknown" >> $GITHUB_OUTPUT
          echo "p95_response_time=unknown" >> $GITHUB_OUTPUT
          echo "p99_response_time=unknown" >> $GITHUB_OUTPUT
          echo "throughput=unknown" >> $GITHUB_OUTPUT
          echo "performance_gate_passed=false" >> $GITHUB_OUTPUT

      - name: Upload JMeter Reports and Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jmeter-load-test-results
          path: |
            jmeter-results/
            youtube-blog-converter-load-test.jmx
            jmeter.log
          retention-days: 30

      - name: Create Comprehensive Load Test Summary
        if: always()
        run: |
          cat > load-test-summary.md << 'EOF'
          # ⚡ Load Test Results Summary

          ## Test Configuration
          - **Target Environment:** Green Environment
          - **Target URL:** ${{ env.GREEN_URL }}
          - **Virtual Users:** ${{ env.JMETER_THREADS }}
          - **Ramp-up Period:** ${{ env.JMETER_RAMPUP }} seconds
          - **Test Duration:** ${{ env.JMETER_DURATION }} seconds
          - **Test Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          ## Performance Metrics
          | Metric | Value | Status |
          |--------|-------|--------|
          | Total Requests | ${{ steps.analyze-results.outputs.total_samples || 'N/A' }} | ℹ️ |
          | Failed Requests | ${{ steps.analyze-results.outputs.errors || 'N/A' }} | ${{ steps.analyze-results.outputs.errors == '0' && '✅' || '⚠️' }} |
          | Error Rate | ${{ steps.analyze-results.outputs.error_rate || 'N/A' }}% | ${{ steps.analyze-results.outputs.performance_gate_passed == 'true' && '✅' || '❌' }} |
          | Average Response Time | ${{ steps.analyze-results.outputs.avg_response_time || 'N/A' }}ms | ${{ steps.analyze-results.outputs.performance_gate_passed == 'true' && '✅' || '❌' }} |
          | 95th Percentile | ${{ steps.analyze-results.outputs.p95_response_time || 'N/A' }}ms | ${{ steps.analyze-results.outputs.performance_gate_passed == 'true' && '✅' || '❌' }} |
          | 99th Percentile | ${{ steps.analyze-results.outputs.p99_response_time || 'N/A' }}ms | ℹ️ |
          | Throughput | ${{ steps.analyze-results.outputs.throughput || 'N/A' }} req/sec | ℹ️ |

          ## Performance Gate
          **Status:** ${{ steps.analyze-results.outputs.performance_gate_passed == 'true' && '✅ PASSED' || '❌ FAILED' }}

          ### Gate Criteria
          - Error Rate: < 1.0%
          - Average Response Time: < 2000ms
          - 95th Percentile Response Time: < 5000ms

          ## Test Execution Status
          **JMeter Execution:** ${{ steps.run-jmeter.outputs.jmeter_success == 'true' && '✅ Success' || '❌ Failed' }}
          **Environment Ready:** ${{ steps.verify-environment.outputs.environment_ready == 'true' && '✅ Yes' || '❌ No' }}

          ## Recommendations
          ${{ steps.analyze-results.outputs.performance_gate_passed == 'true' &&
             '### ✅ Performance Acceptable
             - Application performed within acceptable limits
             - Ready for production traffic
             - Continue monitoring performance in production
             - Consider capacity planning for future growth' ||
             '### ❌ Performance Issues Detected
             - Review application performance bottlenecks
             - Consider scaling resources (CPU, memory, replicas)
             - Optimize database queries and external API calls
             - Re-run load tests after optimizations' }}

          ## Artifacts
          - **JTL Results:** `jmeter-results/results.jtl`
          - **HTML Report:** `jmeter-results/html-report/`
          - **Test Plan:** `youtube-blog-converter-load-test.jmx`
          - **JMeter Logs:** `jmeter.log`

          *This report was automatically generated by the CI/CD pipeline.*
          EOF

          cat load-test-summary.md

  swap-to-production:
    name: Swap Green to Production
    needs: [jmeter-load-test]
    runs-on: ubuntu-latest
    environment:
      name: azure-production

    steps:
      - name: Login to Azure
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Perform Blue-Green Swap
        run: |
          echo "🔄 Swapping Green slot to Production..."
          az webapp deployment slot swap \
            --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
            --name ${{ secrets.AZURE_WEBAPP_NAME }} \
            --slot green \
            --target-slot production

      - name: Verify Production Health
        run: |
          echo "⏳ Verifying production after swap..."
          sleep 30  # Wait for swap to complete

          PROD_URL="https://yt-agent-h5hjdchdf0byh7fc.centralus-01.azurewebsites.net"

          for i in {1..10}; do
            response=$(curl -s -o /dev/null -w "%{http_code}" $PROD_URL/health || echo "000")
            if [ "$response" = "200" ]; then
              echo "✅ Production is healthy after swap"
              break
            fi
            echo "Attempt $i/10: Health check returned $response"
            sleep 5
          done

      - name: Slack Notification - Production Deployment
        uses: act10ns/slack@v2.1.0
        with:
          status: ${{ job.status }}
          webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
          message: |
            🚀 **Production Deployment Complete**

            **Version:** ${{ github.sha }}
            **Production URL:** https://yt-agent-h5hjdchdf0byh7fc.centralus-01.azurewebsites.net
            **Status:** ${{ job.status }}

            Blue-Green swap completed successfully.
            Previous production version is now in Green slot as rollback.

  rollback-if-needed:
    name: Automated Rollback
    needs: [swap-to-production]
    if: failure()
    runs-on: ubuntu-latest

    steps:
      - name: Login to Azure
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Rollback to Previous Version
        run: |
          echo "⚠️ Issues detected, rolling back to previous version..."
          az webapp deployment slot swap \
            --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
            --name ${{ secrets.AZURE_WEBAPP_NAME }} \
            --slot green \
            --target-slot production
          echo "✅ Rollback completed"

      - name: Create Rollback Issue
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🔄 Automatic Rollback Performed',
              body: `An automatic rollback was performed due to deployment failure.

              **Details:**
              - Workflow: ${{ github.workflow }}
              - Run: ${{ github.run_number }}
              - Commit: ${{ github.sha }}
              - Time: ${new Date().toISOString()}

              Please investigate the issue before attempting another deployment.`,
              labels: ['bug', 'deployment-rollback', 'high-priority']
            });

      - name: Slack Alert - Rollback
        uses: act10ns/slack@v2.1.0
        with:
          status: 'failure'
          webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
          message: |
            🔄 **AUTOMATIC ROLLBACK PERFORMED**

            Production deployment failed and was automatically rolled back.
            Previous stable version has been restored.

            **Failed Commit:** ${{ github.sha }}

            Please check logs: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

  # Optional: Clean up old Green slot resources
  cleanup-green-slot:
    name: Stop Green Slot (Save Costs)
    needs: [swap-to-production]
    if: success()
    runs-on: ubuntu-latest

    steps:
      - name: Login to Azure
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Stop Green Slot to Save Costs
        run: |
          echo "⏸️ Stopping Green slot to save costs after successful deployment..."
          echo "ℹ️ Note: Green slot will be automatically restarted on next deployment"

          az webapp stop \
            --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} \
            --name ${{ secrets.AZURE_WEBAPP_NAME }} \
            --slot green

          echo "✅ Green slot stopped successfully"
          echo "💡 Cost optimization: Green slot is now stopped to reduce Azure costs"
          echo "🔄 Next deployment will automatically restart the Green slot before deploying"

      - name: Log Cost Saving Action
        run: |
          echo "💰 Cost Optimization Summary:"
          echo "  - Green slot has been stopped to save Azure compute costs"
          echo "  - Estimated cost savings: ~50% of green slot compute charges"
          echo "  - Next deployment will automatically restart the green slot"
          echo "  - No manual intervention required for future deployments"
          echo ""
          echo "📋 Deployment Cycle:"
          echo "  1. Code push → Pipeline starts"
          echo "  2. Green slot automatically restarted (if stopped)"
          echo "  3. New version deployed to Green slot"
          echo "  4. Testing (Security + Performance) on Green slot"
          echo "  5. Blue-Green swap to Production"
          echo "  6. Green slot stopped again to save costs"

  # ===========================
  # NOTIFICATIONS
  # ===========================

  notify:
      name: Pipeline Status Notification
      if: always()
      runs-on: ubuntu-latest
      needs:
        - lint-and-format
        - security-scan
        - build-and-test
        - generate-sbom
        - sonarqube-scan
        - docker-build
        - deploy-to-dockerhub
        - sign-container
        - deploy-to-azure-green
        - validate-green-deployment
        - owasp-zap-baseline-scan
        - jmeter-load-test
        - swap-to-production
        - cleanup-green-slot

      steps:
        - name: Send Pipeline Status
          uses: act10ns/slack@v2.1.0
          with:
            status: ${{ (contains(needs.*.result, 'failure') && 'failure') || 'success' }}
            webhook-url: ${{ env.SLACK_WEBHOOK_URL }}
            message: |
              **CI/CD Pipeline Complete**

              **Branch:** ${{ github.ref_name }}
              **Commit:** ${{ github.sha }}
              **Status:** ${{ (contains(needs.*.result, 'failure') && '❌ Failed') || '✅ Success' }}

              **Details:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}